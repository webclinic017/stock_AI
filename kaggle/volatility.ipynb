{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "volatility.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPpYK7dRhZ6i1jiTAtIoP9Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuzukiRyotaro1998/stock_AI/blob/main/kaggle/volatility.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Kjl1gwCIesl"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import glob\n",
        "import os\n",
        "import gc\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from sklearn import preprocessing, model_selection\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "\n",
        "path_root = '../input/optiver-realized-volatility-prediction'\n",
        "path_data = '../input/optiver-realized-volatility-prediction'\n",
        "path_submissions = '/'\n",
        "\n",
        "target_name = 'target'\n",
        "scores_folds = {}\n",
        "\n",
        "def log_return(list_stock_prices):\n",
        "    return np.log(list_stock_prices).diff()\n",
        "# デフォルトは.diff()→.pct_change()に変更\n",
        "\n",
        "def log_percent_return(list_stock_prices,b):\n",
        "    return np.log(list_stock_prices).pct_change(periods=b)\n",
        "\n",
        "def realized_volatility(series_log_return):\n",
        "    return np.sqrt(np.sum(series_log_return**2))\n",
        "\n",
        "def rmspe(y_true, y_pred):\n",
        "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
        "\n",
        "def get_stock_stat(stock_id : int, dataType = 'train'):\n",
        "    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n",
        "    \n",
        "    #Book features\n",
        "    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet/stock_id={}/'.format(dataType, stock_id)))\n",
        "    df_book = df_book.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n",
        "    df_book['stock_id'] = stock_id\n",
        "    cols = key + [col for col in df_book.columns if col not in key]\n",
        "    df_book = df_book[cols]\n",
        "    \n",
        "    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] +\n",
        "                                    df_book['ask_price1'] * df_book['bid_size1']) / (df_book['bid_size1'] + df_book['ask_size1'])\n",
        "    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] +\n",
        "                                    df_book['ask_price2'] * df_book['bid_size2']) / (df_book['bid_size2'] + df_book['ask_size2'])\n",
        "    df_book['log_return1'] = df_book.groupby(by = ['time_id'])['wap1'].apply(log_return).fillna(0)\n",
        "    df_book['log_return2'] = df_book.groupby(by = ['time_id'])['wap2'].apply(log_return).fillna(0)\n",
        "    \n",
        "    features_to_apply_realized_volatility = ['log_return'+str(i+1) for i in range(2)]\n",
        "    stock_stat = df_book.groupby(by = ['stock_id', 'time_id'])[features_to_apply_realized_volatility]\\\n",
        "                        .agg(realized_volatility).reset_index()\n",
        "#     #Trade features\n",
        "    trade =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet/stock_id={}'.format(dataType, stock_id)))\n",
        "    trade = trade.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n",
        "    trade_stat = trade.copy()\n",
        "    trade_stat['stock_id'] = stock_id\n",
        "    cols = key + [col for col in trade_stat.columns if col not in key]\n",
        "    trade_stat = trade_stat[cols]\n",
        "    trade_stat['trade_log_return1'] = trade_stat.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n",
        "    trade_stat = trade_stat.groupby(by = ['stock_id', 'time_id'])[['trade_log_return1']]\\\n",
        "                           .agg(realized_volatility).reset_index()\n",
        "    #Joining book and trade features\n",
        "    stock_stat = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n",
        "    \n",
        "    \n",
        "# ############################ryotaroの特徴量##################################################\n",
        "    \n",
        "    df = df_book.copy()\n",
        "    # 高値と安値の比率の対数\n",
        "    df['high_low_ratio'] = np.log(df['bid_price1'] / df['ask_price1'])\n",
        "    # 高値と安値の比率の対数の移動平均\n",
        "    for i in [5,10,15,20,30,40]:\n",
        "        #using center=false to assign values on window's last row\n",
        "        df['high_low_ratio_'+str(i)+'_sma'] = df.groupby(by=['time_id'])['high_low_ratio'].transform(lambda x:  x.rolling(i, center=False).mean())\n",
        "        df['high_low_ratio_'+str(i)+'_sma_ratio'] = df.groupby(by = ['time_id'])['high_low_ratio_'+str(i)+'_sma'].apply(log_return).fillna(0)\n",
        "      \n",
        "    for i in[5,10,15,20,30,40]:\n",
        "        df['today_by_high_low_ratio_'+str(i)+'_sma_ratio']= df['wap1']/df['high_low_ratio_'+str(i)+'_sma'] \n",
        "        \n",
        "    \n",
        "    # Commodity Channel Index in 24 days\n",
        "    df['typical_price'] = (df['bid_price1'] + df['ask_price1'] + df['wap1']) / 3\n",
        "    df['typical_price_log_return'] = df.groupby(by = ['time_id'])['typical_price'].apply(log_return).fillna(0)\n",
        "    \n",
        "    # ADOSC\n",
        "    df['adosc'] = ((2 * df['wap1'] - df['bid_price1'] - df['ask_price1']) / (df['bid_price1'] - df['ask_price1'])) * (df['bid_size1'] + df['ask_size1'])/2\n",
        "    df['adosc'] = df['adosc'].cumsum()\n",
        "    df['adosc_log_return'] = df.groupby(by = ['time_id'])['adosc'].apply(log_return).fillna(0)\n",
        "    \n",
        "    df['adosc-ema3'] = df['adosc'].ewm(span=3, adjust=False).mean()\n",
        "    df['adosc-ema10'] = df['adosc'].ewm(span=10, adjust=False).mean()\n",
        "    df['adosc-SG'] = np.where((df['adosc-ema3'] - df['adosc-ema10']) > 0, 1, -1)\n",
        "    \n",
        "    features_ryotaro = key + ['typical_price','adosc','adosc_log_return','typical_price_log_return'] \n",
        "    df = df[features_ryotaro]\n",
        "#     df = df.groupby(by = ['stock_id','time_id'], as_index=False).nth(-1)\n",
        "    df = df.groupby(by = ['stock_id', 'time_id'])[['typical_price','adosc','adosc_log_return','typical_price_log_return']]\\\n",
        "                        .agg(realized_volatility).reset_index()\n",
        "\n",
        "\n",
        "   \n",
        "    #Joining book and trade features\n",
        "    stock_stat = stock_stat.merge(df, on=['stock_id', 'time_id'], how='left').fillna(-999)\n",
        "\n",
        "#     トレード履歴\n",
        "    df2 = trade.copy()\n",
        "    df2['stock_id'] = stock_id\n",
        "\n",
        "    # simple moving average\n",
        "    for i in [5,10,15,20,30,40]:\n",
        "        df2['sma'+str(i)] = df2.groupby(by=['time_id'])['price'].transform(lambda x:  x.rolling(i, center=False).mean())\n",
        "        df2['sma_log_return'+str(i)] = df2.groupby(by = ['time_id'])['sma'+str(i)].apply(log_return).fillna(0)\n",
        "     \n",
        "        df2['log_percent_return_sma'+str(i)] = df2.groupby(by = ['time_id'])['sma'+str(i)].apply(log_percent_return, b=1).fillna(0)\n",
        "        \n",
        "    for i in [5,10,15,20,30,40]:\n",
        "        df2['today_by_sma'+str(i)+'ratio']= df2['price']/df2['sma'+str(i)]\n",
        "        df2['today_by_sma'+str(i)+'ratio'] = df2.groupby(by = ['time_id'])['today_by_sma'+str(i)+'ratio'].apply(log_return).fillna(0)\n",
        "     \n",
        "    for i in [5,10,15,20,30,40]:\n",
        "        for k in [5,10,15,20,30,40]:\n",
        "            df2['ratio_sma'+str(i)+'_'+str(k)] = df2['sma'+str(i)] / df2['sma'+str(k)]\n",
        "            df2['ratio_sma'+str(i)+'_'+str(k)] = df2.groupby(by = ['time_id'])['ratio_sma'+str(i)+'_'+str(k)].apply(log_return).fillna(0)\n",
        "     \n",
        "\n",
        "    for i in [5,10,15,20,30,40]:\n",
        "        df2['Highest_in_range'+str(i)] = df2.groupby(by=['time_id'])['price'].transform(lambda x:  x.rolling(i, center=False).max())\n",
        "\n",
        "        for m in [5,10,15,20,30,40]:    \n",
        "            df2['Highest'+str(i)+','+str(m)+'days_ago'] = df2['price'] / df2['Highest_in_range'+str(i)].shift(m)\n",
        "            df2['Highest'+str(i)+','+str(m)+'days_ago'] = df2.groupby(by = ['time_id'])['Highest'+str(i)+','+str(m)+'days_ago'].apply(log_return).fillna(0)\n",
        "     \n",
        "        #今日の終値が過去何日間の高音に対してどの程度あるか\n",
        "        df2['Highest'+str(i)] = df2['price'] / df2['Highest_in_range'+str(i)]\n",
        "\n",
        "    # i日前からの価格の変動率_Dena\n",
        "    for i in [5,10,15,20,30,40]:\n",
        "        df2['Adjclose_today_'+str(i)+'daysago_ratio'] = df2.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n",
        "    \n",
        "# #     # i日前からの出来高の変動率_Dena\n",
        "# #     for i in [5,10,15,20,40,61,81,121,161]:\n",
        "# #         df['Volume_'+str(i)+'daysago_ratio'] = df['Volume'] / df['Volume'].shift(i)\n",
        "\n",
        "    features_ryotaro2 = key + ['sma_log_return20','ratio_sma30_15', 'ratio_sma15_20', 'ratio_sma20_5', \n",
        "          'ratio_sma10_20', 'ratio_sma5_15'\n",
        "          ,'Highest20','sma_log_return10'\n",
        "      ,'Highest15','Highest10,20days_ago',\n",
        "\n",
        "'today_by_sma20ratio','today_by_sma15ratio','today_by_sma5ratio','today_by_sma10ratio'\n",
        ",'Adjclose_today_20daysago_ratio','Adjclose_today_15daysago_ratio','Adjclose_today_10daysago_ratio','Adjclose_today_5daysago_ratio',\n",
        "          ]\n",
        "    df2 = df2[features_ryotaro2]\n",
        "#     df2 = df2.groupby(by = ['stock_id','time_id'], as_index=False).nth(-1)\n",
        "    df2 = df2.groupby(by = ['stock_id', 'time_id'])[features_ryotaro2]\\\n",
        "                        .agg(realized_volatility).reset_index()\n",
        "\n",
        "   \n",
        "\n",
        "    #Joining book and trade features\n",
        "    stock_stat = stock_stat.merge(df2, on=['stock_id', 'time_id'], how='left').fillna(-999)\n",
        "    \n",
        "    return stock_stat\n",
        "\n",
        "def get_dataSet(stock_ids : list, dataType = 'train'):\n",
        "    stock_stat_df=pd.DataFrame()\n",
        "    for i in stock_ids:\n",
        "        print(i)\n",
        "        stock_stat = get_stock_stat(i, dataType) \n",
        "        stock_stat_df = stock_stat_df.append(stock_stat)\n",
        "\n",
        "    return stock_stat_df\n",
        "\n",
        "def feval_RMSPE(preds, train_data):\n",
        "    labels = train_data.get_label()\n",
        "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n",
        "\n",
        "params_lgbm = {\n",
        "        'task': 'train',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': 0.01,\n",
        "        'objective': 'regression',\n",
        "        'metric': 'None',\n",
        "        'max_depth': -1,\n",
        "        'n_jobs': -1,\n",
        "        'feature_fraction': 0.7,\n",
        "        'bagging_fraction': 0.7,\n",
        "        'lambda_l2': 1,\n",
        "        'verbose': -1\n",
        "        #'bagging_freq': 5\n",
        "}\n",
        "\n",
        "train = pd.read_csv(os.path.join(path_data, 'train.csv'))\n",
        "print('train')\n",
        "print(train)\n",
        "train_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\n",
        "train = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\n",
        "\n",
        "print('Train shape: {}'.format(train.shape))\n",
        "train.to_csv('/kaggle/working/mycsvfile.csv',index=False)\n",
        "display(train.head(2))\n",
        "\n",
        "test = pd.read_csv(os.path.join(path_data, 'test.csv'))\n",
        "test_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\n",
        "test = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\n",
        "print('Test shape: {}'.format(test.shape))\n",
        "display(test.head(2))\n",
        "\n",
        "cats = []#'stock_id'\n",
        "model_name = 'lgb1'\n",
        "pred_name = 'pred_{}'.format(model_name)\n",
        "\n",
        "\n",
        "features_to_consider = [ 'log_return1', 'log_return2', 'trade_log_return1',\n",
        "                        'typical_price','adosc','adosc_log_return','typical_price_log_return',\n",
        "                        \n",
        "                        'sma_log_return20','ratio_sma30_15', 'ratio_sma15_20', 'ratio_sma20_5', \n",
        "          'ratio_sma10_20', 'ratio_sma5_15'\n",
        "          ,'Highest20','sma_log_return10'\n",
        "      ,'Highest15','Highest10,20days_ago',\n",
        "\n",
        "'today_by_sma20ratio','today_by_sma15ratio','today_by_sma5ratio','today_by_sma10ratio'\n",
        ",'Adjclose_today_20daysago_ratio','Adjclose_today_15daysago_ratio','Adjclose_today_10daysago_ratio','Adjclose_today_5daysago_ratio',\n",
        "          ]\n",
        "\n",
        "\n",
        "print('We consider {} features'.format(len(features_to_consider)))\n",
        "\n",
        "train[pred_name] = 0\n",
        "test['target'] = 0\n",
        "\n",
        "n_folds = 4\n",
        "n_rounds = 5000\n",
        "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2016)\n",
        "scores_folds[model_name] = []\n",
        "counter = 1\n",
        "for dev_index, val_index in kf.split(range(len(train))):\n",
        "    print('CV {}/{}'.format(counter, n_folds))\n",
        "    X_train = train.loc[dev_index, features_to_consider]\n",
        "    y_train = train.loc[dev_index, target_name].values\n",
        "    X_val = train.loc[val_index, features_to_consider]\n",
        "    y_val = train.loc[val_index, target_name].values\n",
        "    import re\n",
        "    X_train = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "    X_val = X_val.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "#     y_train = y_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "#     y_val = y_val.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        " \n",
        " \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    #############################################################################################\n",
        "    #LGB\n",
        "    #############################################################################################\n",
        "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cats, weight=1/np.power(y_train,2))\n",
        "    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cats, weight=1/np.power(y_val,2))\n",
        "    \n",
        "    model = lgb.train(params_lgbm, \n",
        "                      train_data, \n",
        "                      n_rounds, \n",
        "                      valid_sets=val_data, \n",
        "                      feval=feval_RMSPE,\n",
        "                      verbose_eval= 250,\n",
        "                      early_stopping_rounds=500\n",
        "                     )\n",
        "    preds = model.predict(train.loc[val_index, features_to_consider])\n",
        "    train.loc[val_index, pred_name] = preds\n",
        "    score = round(rmspe(y_true = y_val, y_pred = preds),5)\n",
        "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
        "    scores_folds[model_name].append(score)\n",
        "    counter += 1\n",
        "    test[target_name] += model.predict(test[features_to_consider]).clip(0,1e10)\n",
        "del train_data, val_data\n",
        "test[target_name] = test[target_name]/n_folds\n",
        "\n",
        "\n",
        "# このしたの行なぜ2回やるのか\n",
        "score = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\n",
        "print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n",
        "\n",
        "display(test[['row_id', target_name]].head(2))\n",
        "test[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
        "\n",
        "importances = pd.DataFrame({'Feature': model.feature_name(), \n",
        "                            'Importance': model.feature_importance(importance_type='gain')})\n",
        "importances.sort_values(by = 'Importance', inplace=True)\n",
        "importances2 = importances.nlargest(50,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n",
        "importances2[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}