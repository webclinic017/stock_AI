{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimaization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3IiP2Y3IdgL4h+PUNZZks",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuzukiRyotaro1998/stock_AI/blob/main/kaggle/optimaization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcDzBLfVFPBm"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from joblib import Parallel, delayed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "from sklearn.model_selection import KFold\n",
        "import optuna.integration.lightgbm as lgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('max_columns', 300)\n",
        "import matplotlib.pyplot as plt \n",
        "from tqdm import tqdm\n",
        "\n",
        "import optuna\n",
        "\n",
        "from scipy.stats import median_absolute_deviation,skew,kurtosis\n",
        "\n",
        "import cupy as cp\n",
        "import cudf\n",
        "import cuml\n",
        "import glob\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2GZJmyfOrjA"
      },
      "source": [
        "# data directory\n",
        "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
        "\n",
        "# Function to calculate first WAP\n",
        "def calc_wap1(df):\n",
        "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate second WAP\n",
        "def calc_wap2(df):\n",
        "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap\n",
        "\n",
        "def calc_wap3(df):\n",
        "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2']+ df['ask_size2'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate the log of the return\n",
        "# Remember that logb(x / y) = logb(x) - logb(y)\n",
        "def log_return(series):\n",
        "    return np.log(series).diff()\n",
        "\n",
        "# Calculate the realized volatility\n",
        "def realized_volatility(series):\n",
        "    return np.sqrt(np.sum(series**2))\n",
        "\n",
        "# Function to count unique elements of a series\n",
        "def count_unique(series):\n",
        "    return len(np.unique(series))\n",
        "\n",
        "# Function to read our base train and test set\n",
        "def read_train_test():\n",
        "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
        "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
        "    # Create a key to merge with book and trade data\n",
        "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
        "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
        "    print(f'Our training set has {train.shape[0]} rows')\n",
        "    return train, test\n",
        "\n",
        "# Function to preprocess book data (for each stock id)\n",
        "def book_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    # Calculate Wap\n",
        "    df['wap1'] = calc_wap1(df)\n",
        "    df['wap2'] = calc_wap2(df)\n",
        "    df['wap3'] = calc_wap3(df)\n",
        "    # Calculate log returns\n",
        "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return).fillna(0)\n",
        "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return).fillna(0)\n",
        "    df['log_return3'] = df.groupby(['time_id'])['wap2'].apply(log_return).fillna(0)\n",
        "    # Calculate wap balance\n",
        "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
        "    # Calculate spread\n",
        "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
        "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
        "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
        "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
        "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
        "    \n",
        "    # ryotaro_feature\n",
        "    for i in [5,10,15,30,50,100,150,300,450]:\n",
        "        df['sma'+str(i)] = df.groupby(by=['time_id'])['wap1'].transform(lambda x:  x.rolling(i, center=False).mean())\n",
        "     \n",
        "    for i in [5,10,15,30,50]:\n",
        "        df['today_by_sma'+str(i)+'ratio']= df['wap1']/df['sma'+str(i)]\n",
        "        df['today_by_sma_wap2_'+str(i)+'ratio']= df['wap2']/df['sma'+str(i)]\n",
        "        df['today_by_sma'+str(i)+'ratio'] = df.groupby(by = ['time_id'])['today_by_sma'+str(i)+'ratio'].apply(log_return).fillna(0)\n",
        "        df['today_by_sma_wap2_'+str(i)+'ratio'] = df.groupby(by = ['time_id'])['today_by_sma_wap2_'+str(i)+'ratio'].apply(log_return).fillna(0)\n",
        "      \n",
        "    # ADOSC\n",
        "    df['adosc'] = ((2 * df['wap1'] - df['bid_price1'] - df['ask_price1']) / (df['bid_price1'] - df['ask_price1'])) * (df['bid_size1'] + df['ask_size1'])/2\n",
        "    df['adosc_sum'] = df['adosc'].cumsum()\n",
        "    df['adosc_log_return'] = df.groupby(by = ['time_id'])['adosc_sum'].apply(log_return).fillna(0)\n",
        "    \n",
        "    #Highest\n",
        "    for i in [5,10,15,30,50]:\n",
        "        df['Highest_in_range'+str(i)] = df.groupby(by=['time_id'])['wap1'].transform(lambda x:  x.rolling(i, center=False).max())\n",
        "#         for m in [5,10,15,30,50]:    \n",
        "#             df['Highest'+str(i)+','+str(m)+'days_ago'] = df['price'] / df['Highest_in_range'+str(i)].shift(m)\n",
        "#             df['Highest'+str(i)+','+str(m)+'days_ago'] = df.groupby(by = ['time_id'])['Highest'+str(i)+','+str(m)+'days_ago'].apply(log_return).fillna(0)\n",
        "     \n",
        "        #今日の終値が過去何日間の高音に対してどの程度あるか\n",
        "        df['Highest'+str(i)] = df['wap1'] / df['Highest_in_range'+str(i)]\n",
        "\n",
        "#     # i日前からの価格の変動率_Dena\n",
        "#     for i in [5,10,15,20,30,40]:\n",
        "#         df['Adjclose_today_'+str(i)+'daysago_ratio'] = df.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n",
        "    \n",
        "    \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'wap1': [np.sum, np.mean, np.std,np.max, median_absolute_deviation,skew,kurtosis],\n",
        "        'wap2': [np.sum, np.mean, np.std,np.max, median_absolute_deviation,skew,kurtosis],\n",
        "        'wap3': [np.sum, np.mean, np.std,np.max, median_absolute_deviation,skew,kurtosis],\n",
        "        'log_return1': [np.sum, realized_volatility, np.mean, np.std,np.max, median_absolute_deviation,skew,kurtosis],\n",
        "        'log_return2': [np.sum, realized_volatility, np.mean, np.std,np.max, median_absolute_deviation,skew,kurtosis],\n",
        "        'log_return3': [np.sum, realized_volatility, np.mean, np.std,np.max, median_absolute_deviation,skew,kurtosis],\n",
        "        'wap_balance': [np.sum, np.mean, np.std],\n",
        "        'price_spread':[np.sum, np.mean, np.std],\n",
        "        'bid_spread':[np.sum, np.mean, np.std],\n",
        "        'ask_spread':[np.sum, np.mean, np.std],\n",
        "        'ask_price1':[np.sum, np.mean, np.std,np.max, median_absolute_deviation,skew,kurtosis],\n",
        "        'ask_price2':[np.sum, np.mean, np.std,np.max, median_absolute_deviation,skew,kurtosis],\n",
        "        'bid_price1':[np.sum, np.mean, np.std,np.max, median_absolute_deviation,skew,kurtosis],\n",
        "        'bid_price2':[np.sum, np.mean, np.std,np.max, median_absolute_deviation,skew,kurtosis],\n",
        "        'total_volume':[np.sum, np.mean, np.std],\n",
        "        'volume_imbalance':[np.sum, np.mean, np.std],\n",
        "        'today_by_sma5ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'today_by_sma10ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'today_by_sma15ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'today_by_sma30ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "#         'today_by_sma100ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "#         'today_by_sma150ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "#         'today_by_sma300ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "#         'today_by_sma450ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'today_by_sma_wap2_5ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'today_by_sma_wap2_10ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'today_by_sma_wap2_15ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'adosc': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'adosc_sum': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'adosc_log_return': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'Highest5': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'Highest10': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'Highest15': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'Highest30': [np.sum, realized_volatility, np.mean, np.std],\n",
        "    }\n",
        "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window_sort_values(seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df.groupby(['time_id']).rolling(seconds_in_bucket).agg(create_feature_dict).tail().reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_sort_values_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "\n",
        "\n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
        "#     df_feature_100 = get_stats_window(seconds_in_bucket = 100, add_suffix = True)\n",
        "#     df_feature_50 = get_stats_window(seconds_in_bucket = 50, add_suffix = True)\n",
        "#     df_feature_25 = get_stats_window(seconds_in_bucket = 25, add_suffix = True)\n",
        "    \n",
        "#     # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
        "#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
        "#     df_feature = df_feature.merge(df_feature_50, how = 'left', left_on = 'time_id_', right_on = 'time_id__50')\n",
        "#     df_feature = df_feature.merge(df_feature_25, how = 'left', left_on = 'time_id_', right_on = 'time_id__25')\n",
        "# #     # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)# Create row_id so we can merge\n",
        "#     , 'time_id__100', 'time_id__50', 'time_id__25'\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "#     df_feature_sort_values = get_stats_window_sort_values(seconds_in_bucket = 0, add_suffix = False)\n",
        "#     df_feature_450_sort_values = get_stats_window_sort_values(seconds_in_bucket = 450, add_suffix = True)\n",
        "#     df_feature_300_sort_values = get_stats_window_sort_values(seconds_in_bucket = 300, add_suffix = True)\n",
        "#     df_feature_150_sort_values = get_stats_window_sort_values(seconds_in_bucket = 150, add_suffix = True)\n",
        "# #     df_feature_100_sort_values = get_stats_window_sort_values(seconds_in_bucket = 100, add_suffix = True)\n",
        "# #     df_feature_50_sort_values = get_stats_window_sort_values(seconds_in_bucket = 50, add_suffix = True)\n",
        "# #     df_feature_25_sort_values = get_stats_window_sort_values(seconds_in_bucket = 25, add_suffix = True)\n",
        "    \n",
        "#     # Merge all\n",
        "#     df_feature = df_feature.merge(df_feature_sort_values, how = 'left', left_on = 'time_id_', right_on = 'time_id__sort_values')\n",
        "#     df_feature = df_feature.merge(df_feature_450_sort_values, how = 'left', left_on = 'time_id_', right_on = 'time_id__sort_values_450')\n",
        "#     df_feature = df_feature.merge(df_feature_300_sort_values, how = 'left', left_on = 'time_id_', right_on = 'time_id__sort_values_300')\n",
        "#     df_feature = df_feature.merge(df_feature_150_sort_values, how = 'left', left_on = 'time_id_', right_on = 'time_id__sort_values_150')\n",
        "# #     df_feature = df_feature.merge(df_feature_100_sort_values, how = 'left', left_on = 'time_id_', right_on = 'time_id__sort_values_100')\n",
        "# #     df_feature = df_feature.merge(df_feature_50_sort_values, how = 'left', left_on = 'time_id_', right_on = 'time_id__sort_values_50')\n",
        "# #     df_feature = df_feature.merge(df_feature_25_sort_values, how = 'left', left_on = 'time_id_', right_on = 'time_id__sort_values_25')\n",
        "#     # Drop unnecesary time_ids\n",
        "#     df_feature_sort_values.drop(['time_id__sort_values_450', 'time_id__sort_values_300', 'time_id__sort_values_150'], axis = 1, inplace = True)\n",
        "# #    , 'time_id__sort_values_100', 'time_id__sort_values_50', 'time_id__sort_values_25'\n",
        "\n",
        "    \n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
        "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
        " \n",
        "    return df_feature\n",
        "\n",
        "# Function to preprocess trade data (for each stock id)\n",
        "def trade_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
        "\n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'log_return':[realized_volatility],\n",
        "        'price':[realized_volatility,np.std, median_absolute_deviation,skew,kurtosis],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size':[np.sum],\n",
        "        'order_count':[np.mean],\n",
        "    }\n",
        "\n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window_sort_values(seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df.groupby(['time_id']).rolling(seconds_in_bucket).agg(create_feature_dict).tail().reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_sort_values_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
        "    \n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n",
        "    \n",
        "#     # Get the stats for different windows\n",
        "#     df_feature_sort_values = get_stats_window_sort_values(seconds_in_bucket = 0, add_suffix = False)\n",
        "#     df_feature_450_sort_values = get_stats_window_sort_values(seconds_in_bucket = 450, add_suffix = True)\n",
        "#     df_feature_300_sort_values = get_stats_window_sort_values(seconds_in_bucket = 300, add_suffix = True)\n",
        "#     df_feature_150_sort_values = get_stats_window_sort_values(seconds_in_bucket = 150, add_suffix = True)\n",
        "    \n",
        "#     # Merge all\n",
        "#     df_feature = df_feature.merge(df_feature_sort_values, how = 'left', left_on = 'time_id_', right_on = 'time_id__sort_values')\n",
        "#     df_feature = df_feature.merge(df_feature_450_sort_values, how = 'left', left_on = 'time_id_', right_on = 'time_id__sort_values_450')\n",
        "#     df_feature = df_feature.merge(df_feature_300_sort_values, how = 'left', left_on = 'time_id_', right_on = 'time_id__sort_values_300')\n",
        "#     df_feature = df_feature.merge(df_feature_150_sort_values, how = 'left', left_on = 'time_id_', right_on = 'time_id__sort_values_150')\n",
        "#     # Drop unnecesary time_ids\n",
        "#     df_feature_sort_values.drop(['time_id__sort_values_450', 'time_id__sort_values_300', 'time_id__sort_values_150'], axis = 1, inplace = True)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    df_feature = df_feature.add_prefix('trade_')\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
        "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature\n",
        "\n",
        "\n",
        "\n",
        "# Funtion to make preprocessing function in parallel (for each stock id)\n",
        "def preprocessor(list_stock_ids, is_train = True):\n",
        "    \n",
        "    # Parrallel for loop\n",
        "    def for_joblib(stock_id):\n",
        "        # Train\n",
        "        if is_train:\n",
        "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
        "        # Test\n",
        "        else:\n",
        "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
        "    \n",
        "        # Preprocess book and trade data and merge them\n",
        "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
        "        \n",
        "        # Return the merge dataframe\n",
        "        return df_tmp\n",
        "    \n",
        "    stock_dfs = []\n",
        "    PATH = \"/kaggle/input/optiver-realized-volatility-prediction\"\n",
        "    order_book_training = glob.glob(f'{PATH}/book_train.parquet/*/*')\n",
        "    for book_path in tqdm(list(order_book_training)):\n",
        "        try:\n",
        "            stock_id = int(book_path.split(\"=\")[1].split(\"/\")[0])\n",
        "            print(stock_id)\n",
        "            df = for_joblib(stock_id)\n",
        "            stock_dfs.append(df)\n",
        "        except:\n",
        "            print('error')\n",
        "            continue\n",
        "\n",
        "    return pd.concat(stock_dfs)\n",
        "        \n",
        "# # Use parallel api to call paralle for loop\n",
        "#     df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
        "#     # Concatenate all the dataframes that return from Parallel\n",
        "#     df = cudf.concat(df, ignore_index = True)\n",
        "#     return df\n",
        "\n",
        "# Function to calculate the root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqoMHKR1O0jY"
      },
      "source": [
        "def train_and_evaluate(train, test):\n",
        "    # Hyperparammeters (just basic)\n",
        "    params = {\n",
        "      'objective': 'regression', \n",
        "      'metric': 'rmse', \n",
        "      'boosting_type': 'gbdt', \n",
        "      'num_leaves': 100,\n",
        "      'n_jobs': -1,\n",
        "      'learning_rate': 0.1,\n",
        "      'feature_fraction': 0.8,\n",
        "      'bagging_fraction': 0.8,\n",
        "      'verbose': -1\n",
        "    }\n",
        "\n",
        "    feature = ['log_return1_realized_volatility', 'today_by_sma5ratio_realized_volatility', 'today_by_sma10ratio_realized_volatility', 'today_by_sma30ratio_realized_volatility_150', 'log_return1_realized_volatility_150', 'today_by_sma30ratio_realized_volatility', 'today_by_sma15ratio_realized_volatility_150', 'stock_id', 'today_by_sma10ratio_realized_volatility_150', 'today_by_sma15ratio_realized_volatility', 'today_by_sma_wap2_15ratio_realized_volatility', 'log_return1_realized_volatility_300', 'log_return2_realized_volatility_150', 'today_by_sma15ratio_realized_volatility_300', 'log_return2_realized_volatility', 'log_return1_kurtosis', 'log_return2_realized_volatility_300', 'today_by_sma10ratio_realized_volatility_300', 'log_return3_realized_volatility', 'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'price_spread_sum_300', 'today_by_sma_wap2_5ratio_realized_volatility', 'price_spread_sum', 'today_by_sma_wap2_15ratio_realized_volatility_150', 'today_by_sma_wap2_10ratio_realized_volatility', 'price_spread_mean', 'today_by_sma30ratio_realized_volatility_300', 'price_spread_sum_450', 'log_return2_realized_volatility_450', 'ask_spread_mean_450', 'price_spread_sum_150', 'trade_log_return_realized_volatility_300', 'log_return1_kurtosis_150', 'log_return3_realized_volatility_150', 'log_return1_realized_volatility_450', 'price_spread_mean_300', 'trade_log_return_realized_volatility_150', 'volume_imbalance_mean', 'trade_price_realized_volatility_450', 'total_volume_mean', 'wap_balance_sum', 'wap3_median_absolute_deviation_150', 'today_by_sma_wap2_5ratio_realized_volatility_450', 'log_return2_kurtosis_150', 'wap2_std_450', 'wap3_std_450', 'price_spread_mean_150', 'today_by_sma_wap2_10ratio_realized_volatility_150', 'log_return2_kurtosis', 'today_by_sma_wap2_15ratio_realized_volatility_450', 'Highest10_mean', 'trade_price_realized_volatility_300', 'today_by_sma30ratio_std', 'today_by_sma_wap2_5ratio_realized_volatility_150', 'trade_order_count_mean', 'today_by_sma_wap2_5ratio_realized_volatility_300', 'today_by_sma_wap2_10ratio_realized_volatility_450', 'price_spread_std', 'volume_imbalance_std_150', 'log_return3_realized_volatility_450', 'trade_price_realized_volatility_150', 'today_by_sma10ratio_realized_volatility_450', 'today_by_sma_wap2_10ratio_realized_volatility_300', 'today_by_sma5ratio_realized_volatility_300', 'log_return1_std', 'log_return2_std_150', 'today_by_sma_wap2_15ratio_realized_volatility_300', 'price_spread_mean_450', 'log_return3_realized_volatility_300', 'today_by_sma30ratio_realized_volatility_450', 'today_by_sma15ratio_realized_volatility_450', 'ask_spread_mean', 'today_by_sma5ratio_realized_volatility_150', 'log_return1_amax', 'trade_seconds_in_bucket_count_unique_450', 'Highest5_mean', 'Highest5_std_150', 'log_return1_median_absolute_deviation_150', 'log_return1_median_absolute_deviation', 'bid_spread_mean_450', 'trade_seconds_in_bucket_count_unique', 'log_return2_amax_150', 'trade_order_count_mean_450', 'Highest5_sum', 'today_by_sma5ratio_realized_volatility_450', 'wap_balance_sum_150', 'bid_spread_sum', 'log_return1_amax_150', 'log_return2_kurtosis_300', 'total_volume_std', 'wap2_std_300', 'trade_seconds_in_bucket_count_unique_150', 'trade_seconds_in_bucket_count_unique_300', 'adosc_std', 'trade_price_realized_volatility', 'Highest5_mean_300', 'today_by_sma_wap2_10ratio_std_150', 'Highest5_std', 'trade_order_count_mean_300']   \n",
        "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
        "    x = x[feature]\n",
        "    y = train['target']\n",
        "    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
        "    x_test = x_test[feature]\n",
        "    \n",
        "    # Transform stock id to a numeric value\n",
        "    x['stock_id'] = x['stock_id'].astype(int)\n",
        "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
        "\n",
        "    # Create out of folds array\n",
        "    oof_predictions = np.zeros(x.shape[0])\n",
        "    # Create test array to store predictions\n",
        "    test_predictions = np.zeros(x_test.shape[0])\n",
        "    # Create a KFold object\n",
        "    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n",
        "    # Iterate through each fold\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
        "        print(f'Training fold {fold + 1}')\n",
        "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
        "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
        "        # Root mean squared percentage error weights\n",
        "        train_weights = 1 / np.square(y_train)\n",
        "        val_weights = 1 / np.square(y_val)\n",
        "        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
        "        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
        "        model = lgb.train(params = params, \n",
        "                          train_set = train_dataset, \n",
        "                          valid_sets = [train_dataset, val_dataset], \n",
        "                          num_boost_round = 10000, \n",
        "                          early_stopping_rounds = 50, \n",
        "                          verbose_eval = 50,\n",
        "                          feval = feval_rmspe)\n",
        "        # Add predictions to the out of folds array\n",
        "        oof_predictions[val_ind] = model.predict(x_val)\n",
        "        # Predict the test set\n",
        "        test_predictions += model.predict(x_test) / 5\n",
        "        rmspe_score = rmspe(y, oof_predictions)\n",
        "        \n",
        "        best_params = model.params\n",
        "        print(best_params)\n",
        "        \n",
        "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
        "\n",
        "    importances = pd.DataFrame({'Feature': model.feature_name(), \n",
        "                                'Importance': model.feature_importance(importance_type='gain')})\n",
        "    importances.sort_values(by = 'Importance', inplace=True)\n",
        "  \n",
        "    \n",
        "    importances2 = importances.nlargest(100,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n",
        "    print(importances2)\n",
        "    list_sample = importances2['Feature'].to_list()\n",
        "    print(list_sample)\n",
        "    \n",
        "    \n",
        "    \n",
        "    importances3 = importances.nlargest(50,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n",
        "    list_sample = importances3['Feature'].to_list()\n",
        "    print(list_sample)\n",
        "    \n",
        "    \n",
        "    \n",
        "    importances3[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)\n",
        "        # Return test predictions\n",
        "    return test_predictions\n",
        "\n",
        "# Read train and test\n",
        "train, test = read_train_test()\n",
        "\n",
        "# Get unique stock ids \n",
        "train_stock_ids = train['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "train_ = preprocessor(train_stock_ids, is_train = True)\n",
        "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
        "train.to_csv('train.csv',index = False)\n",
        "# print(train)\n",
        "\n",
        "# Get unique stock ids \n",
        "test_stock_ids = test['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "test_ = preprocessor(test_stock_ids, is_train = False)\n",
        "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
        "\n",
        "\n",
        "\n",
        "# Traing and evaluate\n",
        "test_predictions = train_and_evaluate(train, test)\n",
        "# Save test predictions\n",
        "test['target'] = test_predictions\n",
        "test[['row_id', 'target']].to_csv('submission.csv',index = False)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}