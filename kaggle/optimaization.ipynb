{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimaization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOlDcrw5eHWhTjqQ9xjI5dy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuzukiRyotaro1998/stock_AI/blob/main/kaggle/optimaization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcDzBLfVFPBm"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from joblib import Parallel, delayed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('max_columns', 300)\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# !pip install mlflow\n",
        "import optuna\n",
        "import mlflow\n",
        "\n",
        "from mlflow import pyfunc\n",
        "import mlflow.lightgbm\n",
        "\n",
        "from scipy.stats import median_absolute_deviation,skew,kurteosis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2GZJmyfOrjA"
      },
      "source": [
        "# data directory\n",
        "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
        "\n",
        "# Function to calculate first WAP\n",
        "def calc_wap1(df):\n",
        "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate second WAP\n",
        "def calc_wap2(df):\n",
        "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap\n",
        "\n",
        "def calc_wap3(df):\n",
        "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2']+ df['ask_size2'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate the log of the return\n",
        "# Remember that logb(x / y) = logb(x) - logb(y)\n",
        "def log_return(series):\n",
        "    return np.log(series).diff()\n",
        "\n",
        "# Calculate the realized volatility\n",
        "def realized_volatility(series):\n",
        "    return np.sqrt(np.sum(series**2))\n",
        "# Function to calculate the log of the return\n",
        "# Remember that logb(x / y) = logb(x) - logb(y)\n",
        "def log_return(series):\n",
        "    return np.log(series).diff()\n",
        "\n",
        "# Calculate the realized volatility\n",
        "def realized_volatility(series):\n",
        "    return np.sqrt(np.sum(series**2))\n",
        "\n",
        "# Function to count unique elements of a series\n",
        "def count_unique(series):\n",
        "    return len(np.unique(series))\n",
        "\n",
        "# Function to read our base train and test set\n",
        "def read_train_test():\n",
        "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
        "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
        "    # Create a key to merge with book and trade data\n",
        "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
        "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
        "    print(f'Our training set has {train.shape[0]} rows')\n",
        "    return train, test\n",
        "\n",
        "# Function to preprocess book data (for each stock id)\n",
        "def book_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    # Calculate Wap\n",
        "    df['wap1'] = calc_wap1(df)\n",
        "    df['wap2'] = calc_wap2(df)\n",
        "    df['wap3'] = calc_wap3(df)\n",
        "    # Calculate log returns\n",
        "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
        "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
        "    df['log_return3'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
        "    # Calculate wap balance\n",
        "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
        "    # Calculate spread\n",
        "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
        "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
        "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
        "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
        "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
        "    \n",
        "    # ryotaro_feature\n",
        "    for i in [5,10,15,50,100,150,300,450]:\n",
        "        df['sma'+str(i)] = df.groupby(by=['time_id'])['wap1'].transform(lambda x:  x.rolling(i, center=False).mean())\n",
        "     \n",
        "    for i in [5,10,15]:\n",
        "        df['today_by_sma'+str(i)+'ratio']= df['wap1']/df['sma'+str(i)]\n",
        "        df['today_by_sma'+str(i)+'ratio'] = df.groupby(by = ['time_id'])['today_by_sma'+str(i)+'ratio'].apply(log_return).fillna(0)\n",
        "      \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'wap1': [np.sum, np.mean, np.std,np.max, robust.mad],\n",
        "        'wap2': [np.sum, np.mean, np.std,np.max, robust.mad],\n",
        "        'wap3': [np.sum, np.mean, np.std,np.max, robust.mad],\n",
        "        'log_return1': [np.sum, realized_volatility, np.mean, np.std,np.max, robust.mad],\n",
        "        'log_return2': [np.sum, realized_volatility, np.mean, np.std,np.max, robust.mad],\n",
        "        'log_return3': [np.sum, realized_volatility, np.mean, np.std,np.max, robust.mad],\n",
        "        'wap_balance': [np.sum, np.mean, np.std],\n",
        "        'price_spread':[np.sum, np.mean, np.std],\n",
        "        'bid_spread':[np.sum, np.mean, np.std],\n",
        "        'ask_spread':[np.sum, np.mean, np.std],\n",
        "        'ask_price1':[np.sum, np.mean, np.std,np.max, robust.mad],\n",
        "        'ask_price2':[np.sum, np.mean, np.std,np.max, robust.mad],\n",
        "        'bid_price1':[np.sum, np.mean, np.std, robust.mad],\n",
        "        'bid_price2':[np.sum, np.mean, np.std, robust.mad],\n",
        "        'total_volume':[np.sum, np.mean, np.std],\n",
        "        'volume_imbalance':[np.sum, np.mean, np.std],\n",
        "        'today_by_sma5ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'today_by_sma10ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'today_by_sma15ratio': [np.sum, realized_volatility, np.mean, np.std],\n",
        "    }\n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "\n",
        "\n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
        "    df_feature_100 = get_stats_window(seconds_in_bucket = 100, add_suffix = True)\n",
        "    df_feature_50 = get_stats_window(seconds_in_bucket = 50, add_suffix = True)\n",
        "    df_feature_25 = get_stats_window(seconds_in_bucket = 25, add_suffix = True)\n",
        "    \n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
        "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
        "    df_feature = df_feature.merge(df_feature_50, how = 'left', left_on = 'time_id_', right_on = 'time_id__50')\n",
        "    df_feature = df_feature.merge(df_feature_25, how = 'left', left_on = 'time_id_', right_on = 'time_id__25')\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150', 'time_id__100', 'time_id__50', 'time_id__25'], axis = 1, inplace = True)# Create row_id so we can merge\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
        "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
        " \n",
        "    return df_feature\n",
        "\n",
        "# Function to preprocess trade data (for each stock id)\n",
        "def trade_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
        "\n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'log_return':[realized_volatility],\n",
        "        'price':[realized_volatility,np.std, robust.mad],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size':[np.sum],\n",
        "        'order_count':[np.mean],\n",
        "    }\n",
        "\n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
        "    \n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n",
        "    \n",
        "    df_feature = df_feature.add_prefix('trade_')\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
        "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature\n",
        "\n",
        "# Function to get group stats for the stock_id and time_id\n",
        "def get_time_stock(df):\n",
        "    # Get realized volatility columns\n",
        "    vol_cols = ['today_by_sma15ratio_realized_volatility','today_by_sma5ratio_realized_volatility','today_by_sma10ratio_realized_volatility',\n",
        "                'log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return3_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450','log_return3_realized_volatility_450', \n",
        "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return3_realized_volatility_300','log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', 'log_return3_realized_volatility_150', \n",
        "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150',\n",
        "                'trade_price_realized_volatility','trade_price_realized_volatility_450','trade_price_realized_volatility_300','trade_price_realized_volatility_150'\n",
        "               ]\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
        "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_time_id = df.groupby(['stock_id','time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
        "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
        "\n",
        "    # Merge with original dataframe\n",
        "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
        "#     df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
        "    df.drop(['stock_id__stock'], axis = 1, inplace = True)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Funtion to make preprocessing function in parallel (for each stock id)\n",
        "def preprocessor(list_stock_ids, is_train = True):\n",
        "    \n",
        "    # Parrallel for loop\n",
        "    def for_joblib(stock_id):\n",
        "        # Train\n",
        "        if is_train:\n",
        "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
        "        # Test\n",
        "        else:\n",
        "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
        "    \n",
        "        # Preprocess book and trade data and merge them\n",
        "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
        "        \n",
        "        # Return the merge dataframe\n",
        "        return df_tmp\n",
        "    \n",
        "# Use parallel api to call paralle for loop\n",
        "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
        "    # Concatenate all the dataframes that return from Parallel\n",
        "    df = pd.concat(df, ignore_index = True)\n",
        "    return df\n",
        "\n",
        "# Function to calculate the root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqoMHKR1O0jY"
      },
      "source": [
        "\n",
        "\n",
        "def train_and_evaluate(train, test):\n",
        "    # Hyperparammeters (just basic)\n",
        "    params = {\n",
        "      'objective': 'rmse',  \n",
        "      'boosting_type': 'gbdt',\n",
        "      'num_leaves': 100,\n",
        "      'n_jobs': -1,\n",
        "      'learning_rate': 0.1,\n",
        "      'feature_fraction': 0.8,\n",
        "      'bagging_fraction': 0.8,\n",
        "      'verbose': -1\n",
        "    }\n",
        "    \n",
        "\n",
        "         # Split features and target\n",
        "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
        "#     x = x[feature]\n",
        "    y = train['target']\n",
        "    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
        "#     x_test = x_test[feature]\n",
        "    \n",
        "    # Transform stock id to a numeric value\n",
        "    x['stock_id'] = x['stock_id'].astype(int)\n",
        "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
        "\n",
        "    # Create out of folds array\n",
        "    oof_predictions = np.zeros(x.shape[0])\n",
        "    # Create test array to store predictions\n",
        "    test_predictions = np.zeros(x_test.shape[0])\n",
        "    # Create a KFold object\n",
        "    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n",
        "    # Iterate through each fold\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
        "        print(f'Training fold {fold + 1}')\n",
        "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
        "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
        "        # Root mean squared percentage error weights\n",
        "        train_weights = 1 / np.square(y_train)\n",
        "        val_weights = 1 / np.square(y_val)\n",
        "        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
        "        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
        "        model = lgb.train(params = params, \n",
        "                          train_set = train_dataset, \n",
        "                          valid_sets = [train_dataset, val_dataset], \n",
        "                          num_boost_round = 10000, \n",
        "                          early_stopping_rounds = 50, \n",
        "                          verbose_eval = 50,\n",
        "                          feval = feval_rmspe)\n",
        "        # Add predictions to the out of folds array\n",
        "        oof_predictions[val_ind] = model.predict(x_val)\n",
        "        # Predict the test set\n",
        "        test_predictions += model.predict(x_test) / 5\n",
        "        rmspe_score = rmspe(y, oof_predictions)\n",
        "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
        "\n",
        "    importances = pd.DataFrame({'Feature': model.feature_name(), \n",
        "                                'Importance': model.feature_importance(importance_type='gain')})\n",
        "    importances.sort_values(by = 'Importance', inplace=True)\n",
        "  \n",
        "    \n",
        "    importances2 = importances.nlargest(100,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n",
        "    print(importances2)\n",
        "    list_sample = importances2['Feature'].to_list()\n",
        "    print(list_sample)\n",
        "#     importances2.to_csv('Feature.csv',index = False)\n",
        "\n",
        "    importances4 = importances.nlargest(100,'Importance', keep='first').sort_values(by='Importance')\n",
        "    print('特徴量弱い順')\n",
        "    print(importances4)\n",
        "    \n",
        "    \n",
        "    \n",
        "    importances3 = importances.nlargest(50,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n",
        "    list_sample = importances3['Feature'].to_list()\n",
        "    print(list_sample)\n",
        "    \n",
        "    \n",
        "    importances3[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)\n",
        "        # Return test predictions\n",
        "    return test_predictions\n",
        "\n",
        "# Read train and test\n",
        "train, test = read_train_test()\n",
        "\n",
        "# Get unique stock ids \n",
        "train_stock_ids = train['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "train_ = preprocessor(train_stock_ids, is_train = True)\n",
        "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
        "train.to_csv('train.csv',index = False)\n",
        "# print(train)\n",
        "\n",
        "# Get unique stock ids \n",
        "test_stock_ids = test['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "test_ = preprocessor(test_stock_ids, is_train = False)\n",
        "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
        "\n",
        "# Traing and evaluate\n",
        "test_predictions = train_and_evaluate(train, test)\n",
        "# Save test predictions\n",
        "test['target'] = test_predictions\n",
        "test[['row_id', 'target']].to_csv('submission.csv',index = False)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}